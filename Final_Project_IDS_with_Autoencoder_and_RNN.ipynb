{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 6376134,
          "sourceType": "datasetVersion",
          "datasetId": 3674161
        },
        {
          "sourceId": 10842433,
          "sourceType": "datasetVersion",
          "datasetId": 6733479
        },
        {
          "sourceId": 10842458,
          "sourceType": "datasetVersion",
          "datasetId": 6733495
        }
      ],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Final Project - IDS with Autoencoder and RNN",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fikrinotes/LSTM-IDS/blob/main/Final_Project_IDS_with_Autoencoder_and_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "9XCMNofFIzcm"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "dataset_path = kagglehub.dataset_download('chethuhn/network-intrusion-dataset')\n",
        "model_path = kagglehub.dataset_download('fikrimulyanasetiawan/rnn-model')\n",
        "encoder_path = kagglehub.dataset_download('fikrimulyanasetiawan/encoder')\n",
        "\n",
        "print('Data source import complete. \\n')\n",
        "print(\"Information about your data sources:\")\n",
        "print(f\"Dataset path: {dataset_path}\")\n",
        "print(f\"Model path: {model_path}\")\n",
        "print(f\"Encoder path: {encoder_path}\")\n"
      ],
      "metadata": {
        "id": "rb6aowwhIzcp"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intrusion Detection System"
      ],
      "metadata": {
        "id": "SbXVED8lIzcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "A31FOKywIzcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-AdAG5TOIzct"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-15T08:03:57.690893Z",
          "iopub.execute_input": "2025-03-15T08:03:57.691434Z",
          "iopub.status.idle": "2025-03-15T08:03:57.694929Z",
          "shell.execute_reply.started": "2025-03-15T08:03:57.691406Z",
          "shell.execute_reply": "2025-03-15T08:03:57.69418Z"
        },
        "id": "HJhHWwfYIzcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "XGxuqkVPSlWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk membaca dan preprocessing setiap file\n",
        "def read_and_clean_file(file_path):\n",
        "    print(f\"Membaca file: {file_path}\")\n",
        "    df = pd.read_csv(file_path, low_memory=False, sep=\",\")\n",
        "\n",
        "    # Bersihkan nama kolom dari whitespace\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    # Hapus kolom yang tidak diperlukan\n",
        "    redundant_column = ['Flow ID', 'Source IP', 'Source Port', 'Destination IP',\n",
        "                 'Destination Port', 'Protocol', 'Timestamp']\n",
        "    df = df.drop(redundant_column, axis=1, errors='ignore')\n",
        "\n",
        "    # drop baris yang tidak punya label\n",
        "    df.dropna(subset = ['Label'], inplace=True)\n",
        "\n",
        "    # Handling missing values dan infinite values\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Baca semua file CSV dari folder\n",
        "data1 = dataset_path + \"/Monday-WorkingHours.pcap_ISCX.csv\"\n",
        "data2 = dataset_path + \"/Tuesday-WorkingHours.pcap_ISCX.csv\"\n",
        "data3 = dataset_path + \"/Wednesday-workingHours.pcap_ISCX.csv\"\n",
        "data4 = dataset_path + \"/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\"\n",
        "data5 = dataset_path + \"/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\"\n",
        "data6 = dataset_path + \"/Friday-WorkingHours-Morning.pcap_ISCX.csv\"\n",
        "data7 = dataset_path + \"/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\"\n",
        "data8 = dataset_path + \"/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"\n",
        "\n",
        "\n",
        "# Buat list semua dataset yang tersedia\n",
        "all_files = [data2, data3, data4, data5, data6, data7, data8]\n",
        "\n",
        "# Membaca file dan mengkonversi semua data file dari list \"all_files\" menjadi dataframe\n",
        "dataframes = []\n",
        "for file in all_files:\n",
        "    df = read_and_clean_file(file)\n",
        "    dataframes.append(df)\n",
        "    del df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:31:25.991512Z",
          "iopub.execute_input": "2025-06-05T12:31:25.991777Z",
          "iopub.status.idle": "2025-06-05T12:31:32.443529Z",
          "shell.execute_reply.started": "2025-06-05T12:31:25.991754Z",
          "shell.execute_reply": "2025-06-05T12:31:32.44218Z"
        },
        "id": "G-P9CFSGIzcu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggabungkan semua dataframe\n",
        "print(\"Menggabungkan semua file...\")\n",
        "df = pd.concat(dataframes, ignore_index=True)\n",
        "try:\n",
        "    print(\"Semua file dataset berhasil digabungkan!\")\n",
        "except:\n",
        "    print(\"Error! file dataset tidak berhasil digabungkan\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:31:32.444096Z",
          "iopub.status.idle": "2025-06-05T12:31:32.444401Z",
          "shell.execute_reply": "2025-06-05T12:31:32.444288Z"
        },
        "id": "2dR4LqgxIzcw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pembersihan Data Duplikat"
      ],
      "metadata": {
        "id": "2Tjydl0FIzc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ganti nama kolom dengan cara hapus whitespaces\n",
        "col_names = {col: col.strip() for col in df.columns}\n",
        "df.rename(columns = col_names, inplace = True)\n",
        "\n",
        "# informasi data duplikat\n",
        "dups = df[df.duplicated()]\n",
        "print(f'Banyak data duplikat : {len(dups)}')\n",
        "print(f'Banyak data sebelum duplikat : {df.shape[0]}')\n",
        "\n",
        "print(\"menghapus data duplikat...\")\n",
        "\n",
        "# Hapus data duplikat\n",
        "df.drop_duplicates(inplace = True)\n",
        "print(\"data duplikat selesai dihapus!\")\n",
        "df.shape\n",
        "print(f\"banyak data setelah data duplikat dihapus : {df.shape[0]}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "aTc5KAeHIzc1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persiapan Label Kelas untuk Klasifikasi Biner"
      ],
      "metadata": {
        "id": "QXrQ-lRWUAan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# konversi semua label selain BENIGN jadi ATTACK\n",
        "df[\"Label\"] = df[\"Label\"].where(df[\"Label\"] == \"BENIGN\", \"ATTACK\")\n",
        "print(\"Informasi Kelas : \")\n",
        "df[\"Label\"].unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "2BQYHVp6Izc2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Informasi Umum Dataset"
      ],
      "metadata": {
        "id": "0OLQFu6TIzc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan informasi dataset\n",
        "print(\"\\nInformasi Dataset:\")\n",
        "print(f\"\\nJumlah total data: {len(df)}\")\n",
        "print(f\"Jumlah fitur : {len(df.columns)}\")\n",
        "print(\"\\nDistribusi Label sebelum preprocessing:\")\n",
        "\n",
        "# tabel distribusi label\n",
        "def create_distribution_table(df):\n",
        "    label_dist = pd.DataFrame(df['Label'].value_counts())\n",
        "    label_dist['percentage'] = df['Label'].value_counts()/len(df)\n",
        "    return label_dist\n",
        "\n",
        "create_distribution_table(df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "zlc3i5stIzc2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "trusted": true,
        "id": "iOSrfGf2Izc2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan informasi dataset\n",
        "print(\"\\nInformasi Dataset:\")\n",
        "print(f\"\\nJumlah total data: {len(df)}\")\n",
        "print(f\"Jumlah fitur : {len(df.columns)}\")\n",
        "print(\"\\nDistribusi Label setelah preprocessing:\")\n",
        "\n",
        "create_distribution_table(df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "n2dcuIXQIzc2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pemisahan Data Fitur (X) dan Ouput (y)"
      ],
      "metadata": {
        "id": "FiWBz4KAIzc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
        "X = df[numerical_columns]\n",
        "y = df[\"Label\"]\n",
        "print(f\"jumlah fitur : {len(X.columns)}\")\n",
        "print(f\"jumlah label : {len(y.unique())}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "y7NMjQlhIzc3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training-Test Split"
      ],
      "metadata": {
        "id": "mKsCxhm6Izc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tss = TimeSeriesSplit(n_splits=7)\n",
        "print(tss)"
      ],
      "metadata": {
        "trusted": true,
        "id": "AID6zrKGIzc4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
        "train_index, test_index = [], []\n",
        "for i, (train_interval, test_interval) in enumerate(tss.split(X)):\n",
        "    print(f\"fold {i}:\")\n",
        "    print(f\"  Train: index : from {train_interval.min()} up to {train_interval.max()}\")\n",
        "    print(f\"  Test:  index=from {test_interval.min()} up to {test_interval.max()}\")\n",
        "    print(f\"  Jumlah kelas pada training set : {y.iloc[train_interval].nunique()}\")\n",
        "    print(f\"  Jumlah kelas pada testing set : {y.iloc[test_interval].nunique()}\")\n",
        "    train_index, test_index = train_interval, test_interval\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "JZNJJldeIzc4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset dengan stratifikasi\n",
        "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "y_train, y_test  = y.iloc[train_index], y.iloc[test_index]"
      ],
      "metadata": {
        "trusted": true,
        "id": "J8PDNvSgIzc5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7yNX5DojIzc5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "UfQ_8WIjIzc5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformasi Data"
      ],
      "metadata": {
        "id": "LHnJZdvkIzc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean', copy=False)\n",
        "print(\"fitting imputer...\")\n",
        "imputer.fit(X_train)\n",
        "print(\"selesai!\")\n",
        "\n",
        "# scaler\n",
        "scaler = StandardScaler(copy=False)\n",
        "print(\"\\nfitting scaler...\")\n",
        "scaler.fit(X_train)\n",
        "print(\"selesai!\")\n",
        "\n",
        "# label encoder (le)\n",
        "le = LabelEncoder()\n",
        "print(\"\\nfitting label encoder...\")\n",
        "le.fit(y_train.astype(str))\n",
        "print(\"selesai!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "f5DM-4KaIzdC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for i, label in enumerate(le.classes_):\n",
        "    print(f\"i : {i} , label : {label}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "O_cGSTHLIzdE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan informasi kelas\n",
        "print(\"\\nKelas yang terdeteksi:\")\n",
        "for i, label in enumerate(le.classes_):\n",
        "    count = (df[\"Label\"] == i).sum()\n",
        "    print(f\"{label}: {count} samples (encoded as {i})\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "EKtJFw_WIzdE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Label\"].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "vTnZdm1XIzdC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(X, y, scaler, imputer, le):\n",
        "    # Handling missing values untuk dataset training\n",
        "    print(\"\\nMenangani missing values...\")\n",
        "    X = imputer.transform(X)\n",
        "    print(\"selesai!\")\n",
        "\n",
        "    # Normalisasi Data\n",
        "    print(\"\\nMelakukan normalisasi data...\")\n",
        "    X = scaler.transform(X)\n",
        "    print(\"selesai!\")\n",
        "\n",
        "    # Pelabelan Kelas\n",
        "    num_classes = len(le.classes_)\n",
        "    print(\"\\nMelakukan one-hot encoding...\")\n",
        "    y = le.transform(y)\n",
        "    print(\"selesai!\")\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "trusted": true,
        "id": "A6vQ_vQbIzdC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformasi Data Training\n",
        "X_train, y_train = transform_data(X_train, y_train, scaler, imputer, le)\n",
        "\n",
        "# Transformmasi data testing\n",
        "X_test, y_test = transform_data(X_test, y_test, scaler, imputer, le)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8BwGxLuvIzdD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection"
      ],
      "metadata": {
        "id": "GHDtns0BVSHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: lakukan feature selection menggunakan anova f-test\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Lakukan feature selection menggunakan ANOVA F-test\n",
        "# Pilih k fitur terbaik, misalnya k=15\n",
        "k = 20\n",
        "selector = SelectKBest(score_func=f_classif, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "\n",
        "# Dapatkan nama fitur yang terpilih\n",
        "selected_features_indices = selector.get_support(indices=True)\n",
        "selected_feature_names = X.columns[selected_features_indices]\n",
        "\n",
        "print(f\"\\nFitur yang terpilih menggunakan ANOVA F-test (k={k}):\")\n",
        "selected_feature_names"
      ],
      "metadata": {
        "id": "Ap1AxZuQVW-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select feature untuk data testing juga\n",
        "X_test_selected = selector.transform(X_test)"
      ],
      "metadata": {
        "id": "5S3u7iMg_hz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: tampilkan score untuk tiap fitur yang terpilih beserta dengan nama fiturnya\n",
        "\n",
        "# Dapatkan skor untuk setiap fitur\n",
        "scores = selector.scores_[selected_features_indices]\n",
        "\n",
        "# Tampilkan nama fitur dan skornya\n",
        "print(\"\\nSkor untuk fitur yang terpilih:\")\n",
        "for feature, score in zip(selected_feature_names, scores):\n",
        "    print(f\"{feature}: {score}\")"
      ],
      "metadata": {
        "id": "VFeI3yIE5HoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Konstruksi Model Autoencoder (*Batal*)"
      ],
      "metadata": {
        "id": "9JC-Y5RFIzdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Membuat Autoencoder\n",
        "# input_dim = X_train.shape[1]\n",
        "# encoding_dim = 40  # Meningkatkan dimensi encoding karena data lebih kompleks. saran : 64\n",
        "\n",
        "# # Encoder\n",
        "# input_layer = layers.Input(shape=(input_dim,))\n",
        "\n",
        "# encoded = layers.Dense(256, activation='relu')(input_layer)\n",
        "# encoded = layers.BatchNormalization()(encoded)\n",
        "# encoded = layers.Dropout(0.2)(encoded)\n",
        "\n",
        "# encoded = layers.Dense(128, activation='relu')(encoded)\n",
        "# encoded = layers.BatchNormalization()(encoded)\n",
        "# encoded = layers.Dropout(0.2)(encoded)\n",
        "\n",
        "# encoded = layers.Dense(64, activation='relu')(encoded)\n",
        "# encoded = layers.BatchNormalization()(encoded)\n",
        "# encoded = layers.Dropout(0.2)(encoded)\n",
        "\n",
        "# # Bottleneck\n",
        "# encoded = layers.Dense(encoding_dim, activation='linear')(encoded)\n",
        "\n",
        "# # Decoder\n",
        "# decoded = layers.Dense(64, activation='leaky_relu')(encoded) # coba leaky_relu\n",
        "# decoded = layers.BatchNormalization()(decoded)\n",
        "# decoded = layers.Dropout(0.2)(decoded)\n",
        "\n",
        "# decoded = layers.Dense(128, activation='leaky_relu')(decoded)\n",
        "# decoded = layers.BatchNormalization()(decoded)\n",
        "# decoded = layers.Dropout(0.2)(decoded)\n",
        "\n",
        "# decoded = layers.Dense(256, activation='leaky_relu')(decoded)\n",
        "# decoded = layers.BatchNormalization()(decoded)\n",
        "# decoded = layers.Dropout(0.2)(decoded)\n",
        "\n",
        "# decoded = layers.Dense(input_dim, activation='linear')(decoded)\n",
        "\n",
        "# # Model Autoencoder\n",
        "# autoencoder = Model(input_layer, decoded)\n",
        "# encoder = Model(input_layer, encoded)\n",
        "\n",
        "# # Compile dengan learning rate yang sesuai\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "# autoencoder.compile(optimizer=optimizer, loss='mse')"
      ],
      "metadata": {
        "trusted": true,
        "id": "L4_fw-SiIzdG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Training Autoencoder dengan early stopping\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_loss',\n",
        "#     patience=5,\n",
        "#     restore_best_weights=True\n",
        "# )\n",
        "\n",
        "# print(\"\\nTraining Autoencoder...\")\n",
        "# # history_autoencoder = autoencoder.fit(X_train, X_train,\n",
        "# #                                     epochs=50,\n",
        "# #                                     batch_size=512,  # Meningkatkan batch size\n",
        "# #                                     shuffle=True,\n",
        "# #                                     validation_split=0.2,\n",
        "# #                                     callbacks=[early_stopping])\n",
        "# history_autoencoder = autoencoder.fit(X_train, X_train,\n",
        "#                                     epochs=200,\n",
        "#                                     batch_size=256,\n",
        "#                                     validation_data=(X_test, X_test))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "jj9OJJW0IzdG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Mendapatkan encoded features\n",
        "# X_train_encoded = encoder.predict(X_train)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mR3ldNNnIzdG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(history_autoencoder.history['loss'], label='Training Loss')\n",
        "# plt.plot(history_autoencoder.history['val_loss'], label='Validation Loss')\n",
        "# plt.title('Autoencoder Training History')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.savefig(\"ae_plot_history\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "KwqMf5PfIzdH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot_model(autoencoder, to_file='model.png')"
      ],
      "metadata": {
        "trusted": true,
        "id": "vl3Hoo6VIzdH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# autoencoder.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qwwwczs7IzdH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Konstruksi Model LSTM"
      ],
      "metadata": {
        "id": "tb98GUScIzdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = X_train_selected.shape[1]\n",
        "num_classes = len(le.classes_)"
      ],
      "metadata": {
        "id": "fYhN7RkHL2hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, targets, timesteps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - timesteps):\n",
        "        X.append(data[i:i+timesteps])  # Ambil blok sekuensial\n",
        "        y.append(targets[i+timesteps]) # Target berikutnya\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Pilih jumlah timestep (contoh: 10 langkah waktu)\n",
        "timesteps = 10\n",
        "\n",
        "# Buat sequence untuk training dan testing\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_selected, y_train, timesteps)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test_selected, y_test, timesteps)"
      ],
      "metadata": {
        "id": "XiXRFuf5K6vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape Training Data:\")\n",
        "print(\"X_train_seq:\", X_train_seq.shape)  # (samples, timesteps, 15)\n",
        "print(\"y_train_seq:\", y_train_seq.shape)  # (samples,)\n",
        "\n",
        "print(\"\\nShape Testing Data:\")\n",
        "print(\"X_test_seq:\", X_test_seq.shape)    # (samples, timesteps, 15)\n",
        "print(\"y_test_seq:\", y_test_seq.shape)    # (samples,)"
      ],
      "metadata": {
        "id": "q5TPOQOuLJsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# set seed\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "x4-XjDyN0YbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat model LSTM untuk multi-kelas dengan data dari Autoencoder\n",
        "rnn_model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(timesteps, num_features)),\n",
        "\n",
        "    layers.LSTM(128, return_sequences=True),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.35),\n",
        "\n",
        "    layers.LSTM(64),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.35),\n",
        "\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile RNN\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "rnn_model.compile(optimizer=optimizer,\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy', 'precision', 'recall'])\n",
        "\n",
        "# Training RNN dengan early stopping\n",
        "early_stopping_rnn = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(\"\\nTraining RNN...\")\n",
        "# history_rnn = rnn_model.fit(X_train, y_train,\n",
        "#                            epochs=100,\n",
        "#                            batch_size=512,\n",
        "#                            validation_data=(X_test, y_test),\n",
        "#                            callbacks=[early_stopping_rnn])\n",
        "history_rnn = rnn_model.fit(X_train_seq, y_train_seq,\n",
        "                           epochs=100,\n",
        "                           batch_size=128,\n",
        "                           validation_data=(X_test_seq, y_test_seq),\n",
        "                           callbacks=[early_stopping_rnn]\n",
        "                           )\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T08:44:35.146128Z",
          "iopub.execute_input": "2025-06-05T08:44:35.146564Z",
          "iopub.status.idle": "2025-06-05T10:45:07.856767Z",
          "shell.execute_reply.started": "2025-06-05T08:44:35.146527Z",
          "shell.execute_reply": "2025-06-05T10:45:07.852644Z"
        },
        "id": "KLeFkMWlIzdI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_rnn.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_rnn.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('LSTM Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig(\"lstm_training_history\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T10:57:59.170654Z",
          "iopub.execute_input": "2025-06-05T10:57:59.170947Z",
          "iopub.status.idle": "2025-06-05T10:57:59.555481Z",
          "shell.execute_reply.started": "2025-06-05T10:57:59.170915Z",
          "shell.execute_reply": "2025-06-05T10:57:59.554555Z"
        },
        "id": "jyY2a5rVIzdI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.044363Z",
          "iopub.status.idle": "2025-06-03T22:45:48.044679Z",
          "shell.execute_reply": "2025-06-03T22:45:48.04456Z"
        },
        "id": "NzipqXWEIzdJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluasi Model pada Data Test"
      ],
      "metadata": {
        "id": "ifYR8AEkIzdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi model\n",
        "y_pred_prob = rnn_model.predict(X_test_seq)\n",
        "y_pred_classes = (y_pred_prob > 0.5).astype(int)\n",
        "y_test_classes = y_test_seq\n",
        "\n",
        "# Tampilkan hasil evaluasi\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes, target_names=le.classes_))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.047023Z",
          "iopub.status.idle": "2025-06-03T22:45:48.04739Z",
          "shell.execute_reply": "2025-06-03T22:45:48.047227Z"
        },
        "id": "5o8NLaZ7IzdK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_rnn.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_rnn.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('LSTM Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig(\"lstm_training_history\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.050746Z",
          "iopub.status.idle": "2025-06-03T22:45:48.051093Z",
          "shell.execute_reply": "2025-06-03T22:45:48.050977Z"
        },
        "id": "u1NybXhNIzdM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model\n",
        "print(\"\\nMenyimpan model...\")\n",
        "# autoencoder.save('autoencoder_model.h5')\n",
        "# encoder.save('encoder_model.h5')\n",
        "rnn_model.save('rnn_model.keras')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.054188Z",
          "iopub.status.idle": "2025-06-03T22:45:48.054483Z",
          "shell.execute_reply": "2025-06-03T22:45:48.054352Z"
        },
        "id": "sTmzzW39IzdM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan label encoder\n",
        "import joblib\n",
        "joblib.dump(le, 'label_encoder.joblib')\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "joblib.dump(imputer, 'imputer.joblib')\n",
        "joblib.dump(selector, 'selector.joblib')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.055424Z",
          "iopub.status.idle": "2025-06-03T22:45:48.055714Z",
          "shell.execute_reply": "2025-06-03T22:45:48.055586Z"
        },
        "id": "S5h3e7YkIzdM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "hxwnosY8IzdN"
      }
    }
  ]
}