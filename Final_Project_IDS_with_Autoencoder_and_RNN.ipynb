{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 6376134,
          "sourceType": "datasetVersion",
          "datasetId": 3674161
        },
        {
          "sourceId": 10842433,
          "sourceType": "datasetVersion",
          "datasetId": 6733479
        },
        {
          "sourceId": 10842458,
          "sourceType": "datasetVersion",
          "datasetId": 6733495
        }
      ],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Final Project - IDS with Autoencoder and RNN",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fikrinotes/LSTM-IDS/blob/main/Final_Project_IDS_with_Autoencoder_and_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "9XCMNofFIzcm"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "dataset_path = kagglehub.dataset_download('chethuhn/network-intrusion-dataset')\n",
        "model_path = kagglehub.dataset_download('fikrimulyanasetiawan/rnn-model')\n",
        "encoder_path = kagglehub.dataset_download('fikrimulyanasetiawan/encoder')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "rb6aowwhIzcp"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_path)"
      ],
      "metadata": {
        "id": "3x4jUsS7K4zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intrusion Detection System"
      ],
      "metadata": {
        "id": "SbXVED8lIzcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "A31FOKywIzcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-AdAG5TOIzct"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-15T08:03:57.690893Z",
          "iopub.execute_input": "2025-03-15T08:03:57.691434Z",
          "iopub.status.idle": "2025-03-15T08:03:57.694929Z",
          "shell.execute_reply.started": "2025-03-15T08:03:57.691406Z",
          "shell.execute_reply": "2025-03-15T08:03:57.69418Z"
        },
        "id": "HJhHWwfYIzcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk membaca dan preprocessing setiap file\n",
        "def read_and_clean_file(file_path):\n",
        "    print(f\"Membaca file: {file_path}\")\n",
        "    df = pd.read_csv(file_path, low_memory=False, sep=\",\")\n",
        "\n",
        "    # Bersihkan nama kolom dari whitespace\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    # Hapus kolom yang tidak diperlukan\n",
        "    redundant = ['Flow ID', 'Source IP', 'Source Port', 'Destination IP',\n",
        "                 'Destination Port', 'Protocol', 'Timestamp']\n",
        "    df = df.drop(redundant, axis=1, errors='ignore')\n",
        "\n",
        "    # drop baris yang tidak punya label\n",
        "    df.dropna(subset = ['Label'], inplace=True)\n",
        "\n",
        "    # Handling missing values dan infinite values\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Baca semua file CSV dari folder\n",
        "folder_path = '/kaggle/input/network-intrusion-dataset'\n",
        "data1 = dataset_path + \"/Monday-WorkingHours.pcap_ISCX.csv\"\n",
        "data2 = dataset_path + \"/Tuesday-WorkingHours.pcap_ISCX.csv\"\n",
        "data3 = dataset_path + \"/Wednesday-workingHours.pcap_ISCX.csv\"\n",
        "data4 = dataset_path + \"/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\"\n",
        "data5 = dataset_path + \"/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\"\n",
        "data6 = dataset_path + \"/Friday-WorkingHours-Morning.pcap_ISCX.csv\"\n",
        "data7 = dataset_path + \"/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\"\n",
        "data8 = dataset_path + \"/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"\n",
        "\n",
        "\n",
        "# Baca semua file CSV dari folder\n",
        "all_files = [data2, data3, data4, data5, data6, data7, data8]\n",
        "\n",
        "# Membaca file dan mengkonversi data file menjadi dataframe\n",
        "dataframes = []\n",
        "for file in all_files:\n",
        "    # file_path = os.path.join(folder_path, file)\n",
        "    df = read_and_clean_file(file)\n",
        "    dataframes.append(df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:31:25.991512Z",
          "iopub.execute_input": "2025-06-05T12:31:25.991777Z",
          "iopub.status.idle": "2025-06-05T12:31:32.443529Z",
          "shell.execute_reply.started": "2025-06-05T12:31:25.991754Z",
          "shell.execute_reply": "2025-06-05T12:31:32.44218Z"
        },
        "id": "G-P9CFSGIzcu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggabungkan semua dataframe\n",
        "print(\"Menggabungkan semua file...\")\n",
        "df = pd.concat(dataframes, ignore_index=True)\n",
        "try:\n",
        "    print(\"Semua file dataset berhasil digabungkan!\")\n",
        "except:\n",
        "    print(\"Error! file dataset tidak berhasil digabungkan\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:31:32.444096Z",
          "iopub.status.idle": "2025-06-05T12:31:32.444401Z",
          "shell.execute_reply": "2025-06-05T12:31:32.444288Z"
        },
        "id": "2dR4LqgxIzcw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = df.copy(deep=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:31:32.445179Z",
          "iopub.status.idle": "2025-06-05T12:31:32.445483Z",
          "shell.execute_reply": "2025-06-05T12:31:32.445362Z"
        },
        "id": "MQ60RrdLIzcx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe['Label'] = dataframe['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:31:33.365743Z",
          "iopub.execute_input": "2025-06-05T12:31:33.366021Z",
          "iopub.status.idle": "2025-06-05T12:31:34.163724Z",
          "shell.execute_reply.started": "2025-06-05T12:31:33.365997Z",
          "shell.execute_reply": "2025-06-05T12:31:34.162872Z"
        },
        "id": "1qXIFgU1Izcx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:31:34.164835Z",
          "iopub.execute_input": "2025-06-05T12:31:34.165102Z",
          "iopub.status.idle": "2025-06-05T12:31:34.171386Z",
          "shell.execute_reply.started": "2025-06-05T12:31:34.165062Z",
          "shell.execute_reply": "2025-06-05T12:31:34.170492Z"
        },
        "id": "FUUcQPqMIzcy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = dataframe.corr(method='pearson')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:31:34.172735Z",
          "iopub.execute_input": "2025-06-05T12:31:34.173012Z",
          "iopub.status.idle": "2025-06-05T12:32:07.256017Z",
          "shell.execute_reply.started": "2025-06-05T12:31:34.172985Z",
          "shell.execute_reply": "2025-06-05T12:32:07.255275Z"
        },
        "id": "sMNFGUHGIzcy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.set_option('display.max_rows', None)\n",
        "# pd.set_option('display.max_columns', None)\n",
        "# pd.set_option('display.width', None)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:32:07.257225Z",
          "iopub.execute_input": "2025-06-05T12:32:07.257528Z",
          "iopub.status.idle": "2025-06-05T12:32:07.261357Z",
          "shell.execute_reply.started": "2025-06-05T12:32:07.25749Z",
          "shell.execute_reply": "2025-06-05T12:32:07.260447Z"
        },
        "id": "YwyK_I3kIzcz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:32:07.262876Z",
          "iopub.execute_input": "2025-06-05T12:32:07.263118Z",
          "iopub.status.idle": "2025-06-05T12:32:07.437285Z",
          "shell.execute_reply.started": "2025-06-05T12:32:07.263072Z",
          "shell.execute_reply": "2025-06-05T12:32:07.436463Z"
        },
        "id": "-MINm2n7Izcz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Korelasi setiap fitur dengan label\n",
        "target_corr = corr_matrix['Label'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Pilih fitur dengan korelasi > threshold (misal: top 20%)\n",
        "threshold = 0.05  # Ambang minimal korelasi\n",
        "high_corr_features = target_corr[target_corr > threshold].index.tolist()\n",
        "\n",
        "# Hapus 'Label' jika termasuk\n",
        "high_corr_features = [f for f in high_corr_features if f != 'Label']\n",
        "\n",
        "print(f\"Selected features: {len(high_corr_features)}\")\n",
        "print(high_corr_features)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:32:07.438417Z",
          "iopub.execute_input": "2025-06-05T12:32:07.438605Z",
          "iopub.status.idle": "2025-06-05T12:32:07.446234Z",
          "shell.execute_reply.started": "2025-06-05T12:32:07.438589Z",
          "shell.execute_reply": "2025-06-05T12:32:07.445501Z"
        },
        "id": "WnPsGZGpIzcz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat matriks korelasi hanya untuk fitur terpilih\n",
        "high_corr_df = dataframe[high_corr_features]\n",
        "feature_corr = high_corr_df.corr().abs()\n",
        "\n",
        "# Identifikasi pasangan fitur dengan korelasi tinggi\n",
        "to_drop = set()\n",
        "for i in range(len(feature_corr.columns)):\n",
        "    for j in range(i):\n",
        "        if feature_corr.iloc[i, j] > 0.8:  # Threshold multikolinearitas\n",
        "            col_i = feature_corr.columns[i]\n",
        "            col_j = feature_corr.columns[j]\n",
        "\n",
        "            # Bandingkan korelasi dengan target, hapus yang lebih rendah\n",
        "            if target_corr[col_i] > target_corr[col_j]:\n",
        "                to_drop.add(col_j)\n",
        "            else:\n",
        "                to_drop.add(col_i)\n",
        "\n",
        "# Final feature selection\n",
        "final_features = [f for f in high_corr_features if f not in to_drop]\n",
        "\n",
        "print(f\"\\nFinal features after multicollinearity check: {len(final_features)}\")\n",
        "print(final_features)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:32:07.44703Z",
          "iopub.execute_input": "2025-06-05T12:32:07.447321Z",
          "iopub.status.idle": "2025-06-05T12:32:07.522025Z",
          "shell.execute_reply.started": "2025-06-05T12:32:07.447294Z",
          "shell.execute_reply": "2025-06-05T12:32:07.521249Z"
        },
        "id": "Ocaonf33Izc0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:32:07.52286Z",
          "iopub.execute_input": "2025-06-05T12:32:07.523143Z",
          "iopub.status.idle": "2025-06-05T12:32:11.146821Z",
          "shell.execute_reply.started": "2025-06-05T12:32:07.52311Z",
          "shell.execute_reply": "2025-06-05T12:32:11.146015Z"
        },
        "id": "5i68jGtRIzc0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['Label']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T12:32:11.147747Z",
          "iopub.execute_input": "2025-06-05T12:32:11.148093Z"
        },
        "id": "BnIxuI_6Izc0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pembersihan data duplikat"
      ],
      "metadata": {
        "id": "2Tjydl0FIzc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ganti nama kolom dengan cara hapus whitespaces\n",
        "col_names = {col: col.strip() for col in df.columns}\n",
        "df.rename(columns = col_names, inplace = True)\n",
        "\n",
        "# informasi data duplikat\n",
        "dups = df[df.duplicated()]\n",
        "print(f'Number of duplicates: {len(dups)}')"
      ],
      "metadata": {
        "trusted": true,
        "id": "aTc5KAeHIzc1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "jvqsPuWHIzc1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Hapus data duplikat\n",
        "df.drop_duplicates(inplace = True)\n",
        "df.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "UXOyMBPmIzc1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Informasi Umum Dataset"
      ],
      "metadata": {
        "id": "0OLQFu6TIzc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan informasi dataset\n",
        "print(\"\\nInformasi Dataset:\")\n",
        "print(f\"\\nJumlah total data: {len(df)}\")\n",
        "print(f\"Jumlah fitur : {len(df.columns)}\")\n",
        "print(\"\\nDistribusi Label sebelum preprocessing:\")\n",
        "\n",
        "# tabel distribusi label\n",
        "def create_distribution_table(df):\n",
        "    label_dist = pd.DataFrame(df['Label'].value_counts())\n",
        "    label_dist['percentage'] = df['Label'].value_counts()/len(df)\n",
        "    return label_dist\n",
        "\n",
        "create_distribution_table(df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "zlc3i5stIzc2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Label\"] = df[\"Label\"].where(df[\"Label\"] == \"BENIGN\", \"ANOMALY\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "2BQYHVp6Izc2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Label\"].unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "oAI0qAvQIzc2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "trusted": true,
        "id": "iOSrfGf2Izc2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan informasi dataset\n",
        "print(\"\\nInformasi Dataset:\")\n",
        "print(f\"\\nJumlah total data: {len(df)}\")\n",
        "print(f\"Jumlah fitur : {len(df.columns)}\")\n",
        "print(\"\\nDistribusi Label setelah preprocessing:\")\n",
        "\n",
        "create_distribution_table(df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "n2dcuIXQIzc2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pemisahan Data Fitur (X) dan Ouput (y)"
      ],
      "metadata": {
        "id": "FiWBz4KAIzc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[final_features].info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "VJrlrO3IIzc3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
        "X = df[final_features]\n",
        "y = df[\"Label\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "y7NMjQlhIzc3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "trusted": true,
        "id": "uZoo32wSIzc3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training-Test Split"
      ],
      "metadata": {
        "id": "mKsCxhm6Izc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tss = TimeSeriesSplit(n_splits=7)\n",
        "print(tss)"
      ],
      "metadata": {
        "trusted": true,
        "id": "AID6zrKGIzc4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
        "train_index, test_index = [], []\n",
        "for i, (train_interval, test_interval) in enumerate(tss.split(X)):\n",
        "    print(f\"fold {i}:\")\n",
        "    print(f\"  Train: index : from {train_interval.min()} up to {train_interval.max()}\")\n",
        "    print(f\"  Test:  index=from {test_interval.min()} up to {test_interval.max()}\")\n",
        "    print(f\"  Jumlah kelas pada training set : {y.iloc[train_interval].nunique()}\")\n",
        "    print(f\"  Jumlah kelas pada testing set : {y.iloc[test_interval].nunique()}\")\n",
        "    train_index, test_index = train_interval, test_interval\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "JZNJJldeIzc4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset dengan stratifikasi\n",
        "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "y_train, y_test  = y.iloc[train_index], y.iloc[test_index]"
      ],
      "metadata": {
        "trusted": true,
        "id": "J8PDNvSgIzc5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7yNX5DojIzc5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "UfQ_8WIjIzc5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformasi Data"
      ],
      "metadata": {
        "id": "LHnJZdvkIzc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew, kurtosis"
      ],
      "metadata": {
        "trusted": true,
        "id": "vBxhBhAGIzdB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penanganan Missing Values, Normalisasi Data dan Label Encoding"
      ],
      "metadata": {
        "id": "Cvj9-gSGIzdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean', copy=False)\n",
        "print(\"fitting imputer...\")\n",
        "imputer.fit(X_train)\n",
        "print(\"selesai!\")\n",
        "\n",
        "# scaler\n",
        "scaler = StandardScaler(copy=False)\n",
        "print(\"\\nfitting scaler...\")\n",
        "scaler.fit(X_train)\n",
        "print(\"selesai!\")\n",
        "\n",
        "# label encoder (le)\n",
        "le = LabelEncoder()\n",
        "print(\"\\nfitting label encoder...\")\n",
        "le.fit(y_train.astype(str))\n",
        "print(\"selesai!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "f5DM-4KaIzdC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Label\"].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "vTnZdm1XIzdC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(X, y, scaler, imputer, le):\n",
        "    # Handling missing values untuk dataset training\n",
        "    print(\"\\nMenangani missing values...\")\n",
        "    X = imputer.transform(X)\n",
        "    print(\"selesai!\")\n",
        "\n",
        "    # Normalisasi Data\n",
        "    print(\"\\nMelakukan normalisasi data...\")\n",
        "    X = scaler.transform(X)\n",
        "    print(\"selesai!\")\n",
        "\n",
        "    # One-hot encoding untuk target (karena multi-kelas)\n",
        "    num_classes = len(le.classes_)\n",
        "    print(\"\\nMelakukan one-hot encoding...\")\n",
        "    y = le.transform(y)\n",
        "    y = tf.keras.utils.to_categorical(y, num_classes)\n",
        "    print(\"selesai!\")\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "trusted": true,
        "id": "A6vQ_vQbIzdC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "trusted": true,
        "id": "lLs7kRWUIzdD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(le.classes_)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xkHObwJZIzdD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes"
      ],
      "metadata": {
        "trusted": true,
        "id": "BTx9Izb4IzdD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformasi Data Training\n",
        "X_train, y_train = transform_data(X_train, y_train, scaler, imputer, le)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8BwGxLuvIzdD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "le.classes_"
      ],
      "metadata": {
        "trusted": true,
        "id": "FUJhtiWHIzdE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "trusted": true,
        "id": "-rDPqzqdIzdE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for i, label in enumerate(le.classes_):\n",
        "    print(f\"i : {i} , label : {label}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "O_cGSTHLIzdE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan informasi kelas\n",
        "print(\"\\nKelas yang terdeteksi:\")\n",
        "for i, label in enumerate(le.classes_):\n",
        "    count = (df[\"Label\"] == i).sum()\n",
        "    print(f\"{label}: {count} samples (encoded as {i})\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "EKtJFw_WIzdE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penggunaan TPU"
      ],
      "metadata": {
        "id": "F_llmWo-IzdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # detect and init the TPU\n",
        "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
        "\n",
        "# # instantiate a distribution strategy\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "XtSBsSKCIzdF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Konstruksi Model Autoencoder"
      ],
      "metadata": {
        "id": "9JC-Y5RFIzdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "trusted": true,
        "id": "HWy6Y66EIzdF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformmasi data testing\n",
        "X_test, y_test = transform_data(X_test, y_test, scaler, imputer, le)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ehi2E0ANIzdF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Membuat Autoencoder\n",
        "# input_dim = X_train.shape[1]\n",
        "# encoding_dim = 40  # Meningkatkan dimensi encoding karena data lebih kompleks. saran : 64\n",
        "\n",
        "# # Encoder\n",
        "# input_layer = layers.Input(shape=(input_dim,))\n",
        "\n",
        "# encoded = layers.Dense(256, activation='relu')(input_layer)\n",
        "# encoded = layers.BatchNormalization()(encoded)\n",
        "# encoded = layers.Dropout(0.2)(encoded)\n",
        "\n",
        "# encoded = layers.Dense(128, activation='relu')(encoded)\n",
        "# encoded = layers.BatchNormalization()(encoded)\n",
        "# encoded = layers.Dropout(0.2)(encoded)\n",
        "\n",
        "# encoded = layers.Dense(64, activation='relu')(encoded)\n",
        "# encoded = layers.BatchNormalization()(encoded)\n",
        "# encoded = layers.Dropout(0.2)(encoded)\n",
        "\n",
        "# # Bottleneck\n",
        "# encoded = layers.Dense(encoding_dim, activation='linear')(encoded)\n",
        "\n",
        "# # Decoder\n",
        "# decoded = layers.Dense(64, activation='leaky_relu')(encoded) # coba leaky_relu\n",
        "# decoded = layers.BatchNormalization()(decoded)\n",
        "# decoded = layers.Dropout(0.2)(decoded)\n",
        "\n",
        "# decoded = layers.Dense(128, activation='leaky_relu')(decoded)\n",
        "# decoded = layers.BatchNormalization()(decoded)\n",
        "# decoded = layers.Dropout(0.2)(decoded)\n",
        "\n",
        "# decoded = layers.Dense(256, activation='leaky_relu')(decoded)\n",
        "# decoded = layers.BatchNormalization()(decoded)\n",
        "# decoded = layers.Dropout(0.2)(decoded)\n",
        "\n",
        "# decoded = layers.Dense(input_dim, activation='linear')(decoded)\n",
        "\n",
        "# # Model Autoencoder\n",
        "# autoencoder = Model(input_layer, decoded)\n",
        "# encoder = Model(input_layer, encoded)\n",
        "\n",
        "# # Compile dengan learning rate yang sesuai\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "# autoencoder.compile(optimizer=optimizer, loss='mse')"
      ],
      "metadata": {
        "trusted": true,
        "id": "L4_fw-SiIzdG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Training Autoencoder dengan early stopping\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_loss',\n",
        "#     patience=5,\n",
        "#     restore_best_weights=True\n",
        "# )\n",
        "\n",
        "# print(\"\\nTraining Autoencoder...\")\n",
        "# # history_autoencoder = autoencoder.fit(X_train, X_train,\n",
        "# #                                     epochs=50,\n",
        "# #                                     batch_size=512,  # Meningkatkan batch size\n",
        "# #                                     shuffle=True,\n",
        "# #                                     validation_split=0.2,\n",
        "# #                                     callbacks=[early_stopping])\n",
        "# history_autoencoder = autoencoder.fit(X_train, X_train,\n",
        "#                                     epochs=200,\n",
        "#                                     batch_size=256,\n",
        "#                                     validation_data=(X_test, X_test))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "jj9OJJW0IzdG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Mendapatkan encoded features\n",
        "# X_train_encoded = encoder.predict(X_train)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mR3ldNNnIzdG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(history_autoencoder.history['loss'], label='Training Loss')\n",
        "# plt.plot(history_autoencoder.history['val_loss'], label='Validation Loss')\n",
        "# plt.title('Autoencoder Training History')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.savefig(\"ae_plot_history\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "KwqMf5PfIzdH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot_model(autoencoder, to_file='model.png')"
      ],
      "metadata": {
        "trusted": true,
        "id": "vl3Hoo6VIzdH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# autoencoder.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qwwwczs7IzdH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Konstruksi Model LSTM"
      ],
      "metadata": {
        "id": "tb98GUScIzdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "jJfoASPSIzdI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X_train.shape[1]"
      ],
      "metadata": {
        "id": "2IOKJvy-N5Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat model LSTM untuk multi-kelas dengan data dari Autoencoder\n",
        "rnn_model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(input_dim,)), # ini encoding_dim (sebenarnya)\n",
        "    layers.Reshape((input_dim, 1)),\n",
        "    layers.LSTM(256, return_sequences=True),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.LSTM(128),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')  # Output layer untuk multi-kelas, gunakan num_classes utk jumlah neuron\n",
        "]) # jika cuma 2 kelas (seperti deteksi anomali, cukup 1 neuron pada layer output)\n",
        "\n",
        "# Compile RNN\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "rnn_model.compile(optimizer=optimizer,\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "# Training RNN dengan early stopping\n",
        "early_stopping_rnn = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(\"\\nTraining RNN...\")\n",
        "# history_rnn = rnn_model.fit(X_train, y_train,\n",
        "#                            epochs=100,\n",
        "#                            batch_size=512,\n",
        "#                            validation_data=(X_test, y_test),\n",
        "#                            callbacks=[early_stopping_rnn])\n",
        "history_rnn = rnn_model.fit(X_train, y_train,\n",
        "                           epochs=100,\n",
        "                           batch_size=256,\n",
        "                           validation_data=(X_test, y_test)\n",
        "                           )\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T08:44:35.146128Z",
          "iopub.execute_input": "2025-06-05T08:44:35.146564Z",
          "iopub.status.idle": "2025-06-05T10:45:07.856767Z",
          "shell.execute_reply.started": "2025-06-05T08:44:35.146527Z",
          "shell.execute_reply": "2025-06-05T10:45:07.852644Z"
        },
        "id": "KLeFkMWlIzdI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_rnn.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_rnn.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('LSTM Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig(\"lstm_training_history\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-05T10:57:59.170654Z",
          "iopub.execute_input": "2025-06-05T10:57:59.170947Z",
          "iopub.status.idle": "2025-06-05T10:57:59.555481Z",
          "shell.execute_reply.started": "2025-06-05T10:57:59.170915Z",
          "shell.execute_reply": "2025-06-05T10:57:59.554555Z"
        },
        "id": "jyY2a5rVIzdI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat model RNN untuk multi-kelas tanpa data dari Autoencoder\n",
        "# encoding_dim = 78\n",
        "# rnn_model = tf.keras.Sequential([\n",
        "#     layers.Input(shape=(encoding_dim,)),\n",
        "#     layers.Reshape((encoding_dim, 1)),\n",
        "#     layers.LSTM(256, return_sequences=True),\n",
        "#     layers.Dropout(0.4),\n",
        "#     layers.LSTM(128),\n",
        "#     layers.Dropout(0.4),\n",
        "#     layers.Dense(64, activation='relu'),\n",
        "#     layers.Dense(num_classes, activation='softmax')  # Output layer untuk multi-kelas\n",
        "# ])\n",
        "\n",
        "# # Compile RNN\n",
        "# rnn_model.compile(optimizer='adam',\n",
        "#                  loss='categorical_crossentropy',\n",
        "#                  metrics=['accuracy'])\n",
        "\n",
        "# # Training RNN dengan early stopping\n",
        "# early_stopping_rnn = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_accuracy',\n",
        "#     patience=5,\n",
        "#     restore_best_weights=True\n",
        "# )\n",
        "\n",
        "# print(\"\\nTraining RNN...\")\n",
        "# history_rnn = rnn_model.fit(X_train, y_train,\n",
        "#                            epochs=50,\n",
        "#                            batch_size=512,\n",
        "#                            validation_split=0.2,\n",
        "#                            callbacks=[early_stopping_rnn])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.043364Z",
          "iopub.status.idle": "2025-06-03T22:45:48.043632Z",
          "shell.execute_reply": "2025-06-03T22:45:48.043524Z"
        },
        "id": "6xitYOS3IzdJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.044363Z",
          "iopub.status.idle": "2025-06-03T22:45:48.044679Z",
          "shell.execute_reply": "2025-06-03T22:45:48.04456Z"
        },
        "id": "NzipqXWEIzdJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluasi Model pada Data Test"
      ],
      "metadata": {
        "id": "ifYR8AEkIzdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = transform_data(X_test, y_test, scaler, imputer, le)\n",
        "X_test_encoded = encoder.predict(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.045669Z",
          "iopub.status.idle": "2025-06-03T22:45:48.046033Z",
          "shell.execute_reply": "2025-06-03T22:45:48.045833Z"
        },
        "id": "JFhzRjh1IzdK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Evaluasi model\n",
        "y_pred = rnn_model.predict(X_test_encoded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Tampilkan hasil evaluasi\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes, target_names=le.classes_))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.047023Z",
          "iopub.status.idle": "2025-06-03T22:45:48.04739Z",
          "shell.execute_reply": "2025-06-03T22:45:48.047227Z"
        },
        "id": "5o8NLaZ7IzdK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_autoencoder.history['loss'], label='Training Loss')\n",
        "plt.plot(history_autoencoder.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Autoencoder Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_rnn.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_rnn.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('RNN Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.047999Z",
          "iopub.status.idle": "2025-06-03T22:45:48.048619Z",
          "shell.execute_reply": "2025-06-03T22:45:48.048443Z"
        },
        "id": "xfs6JCDrIzdL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_autoencoder.history['loss'], label='Training Loss')\n",
        "plt.plot(history_autoencoder.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Autoencoder Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig(\"ae_plot_history\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.049506Z",
          "iopub.status.idle": "2025-06-03T22:45:48.049903Z",
          "shell.execute_reply": "2025-06-03T22:45:48.049725Z"
        },
        "id": "N5di1_KJIzdL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_rnn.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_rnn.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('LSTM Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig(\"lstm_training_history\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.050746Z",
          "iopub.status.idle": "2025-06-03T22:45:48.051093Z",
          "shell.execute_reply": "2025-06-03T22:45:48.050977Z"
        },
        "id": "u1NybXhNIzdM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot confusion matrix\n",
        "# plt.figure(figsize=(12, 10))\n",
        "# cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
        "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "#             xticklabels=le.classes_,\n",
        "#             yticklabels=le.classes_)\n",
        "# plt.title('Confusion Matrix')\n",
        "# plt.ylabel('True Label')\n",
        "# plt.xlabel('Predicted Label')\n",
        "# plt.xticks(rotation=45)\n",
        "# plt.yticks(rotation=45)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "# plt.savefig(\"confusion_matrix\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.053034Z",
          "iopub.status.idle": "2025-06-03T22:45:48.053319Z",
          "shell.execute_reply": "2025-06-03T22:45:48.053204Z"
        },
        "id": "glRSAW2VIzdM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model\n",
        "print(\"\\nMenyimpan model...\")\n",
        "autoencoder.save('autoencoder_model.h5')\n",
        "encoder.save('encoder_model.h5')\n",
        "rnn_model.save('rnn_model.h5')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.054188Z",
          "iopub.status.idle": "2025-06-03T22:45:48.054483Z",
          "shell.execute_reply": "2025-06-03T22:45:48.054352Z"
        },
        "id": "sTmzzW39IzdM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan label encoder\n",
        "import joblib\n",
        "joblib.dump(le, 'label_encoder.joblib')\n",
        "joblib.dump(scaler, 'scaler.joblib')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T22:45:48.055424Z",
          "iopub.status.idle": "2025-06-03T22:45:48.055714Z",
          "shell.execute_reply": "2025-06-03T22:45:48.055586Z"
        },
        "id": "S5h3e7YkIzdM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "hxwnosY8IzdN"
      }
    }
  ]
}