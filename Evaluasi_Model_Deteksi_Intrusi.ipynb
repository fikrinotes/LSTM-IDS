{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqr4ZHoKwIIwKUPTc2TlW9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fikrinotes/LSTM-IDS/blob/main/Evaluasi_Model_Deteksi_Intrusi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxdfNxQewmtd"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "mDVWyvAR1h7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "rZarePXpEzWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "dataset_path = kagglehub.dataset_download('chethuhn/network-intrusion-dataset')\n",
        "model_path = kagglehub.dataset_download('fikrimulyanasetiawan/rnn-model')\n",
        "encoder_path = kagglehub.dataset_download('fikrimulyanasetiawan/encoder')\n",
        "\n",
        "print('Data source import complete. \\n')\n",
        "print(\"Information about your data sources:\")\n",
        "print(f\"Dataset path: {dataset_path}\")\n",
        "print(f\"Model path: {model_path}\")\n",
        "print(f\"Encoder path: {encoder_path}\")"
      ],
      "metadata": {
        "id": "umCUMze2w2i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "9trpgX7xE7k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "\n",
        "urlretrieve('https://github.com/fikrinotes/LSTM-IDS/raw/refs/heads/main/Model%20v2.3/imputer.joblib', 'imputer.joblib')\n",
        "urlretrieve('https://github.com/fikrinotes/LSTM-IDS/raw/refs/heads/main/Model%20v2.3/label_encoder.joblib', 'label_encoder.joblib')\n",
        "urlretrieve('https://github.com/fikrinotes/LSTM-IDS/raw/refs/heads/main/Model%20v2.3/rnn_model.keras', 'rnn_model.keras')\n",
        "urlretrieve('https://github.com/fikrinotes/LSTM-IDS/raw/refs/heads/main/Model%20v2.3/scaler.joblib', 'scaler.joblib')\n",
        "urlretrieve('https://github.com/fikrinotes/LSTM-IDS/raw/refs/heads/main/Model%20v2.3/selector.joblib', 'selector.joblib')"
      ],
      "metadata": {
        "id": "zOQ32kem5rZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load imputer, labelencoder, rnn_model, scaler dan selector\n",
        "\n",
        "import pickle\n",
        "from joblib import load\n",
        "from tensorflow import keras\n",
        "\n",
        "path = \"/content\"\n",
        "# Load the imputer\n",
        "imputer_file = f\"{path}/imputer.joblib\"\n",
        "imputer = load(imputer_file)\n",
        "\n",
        "# Load the label encoder\n",
        "label_encoder_file = f\"{path}/label_encoder.joblib\"\n",
        "label_encoder = load(label_encoder_file)\n",
        "\n",
        "# Load the scaler\n",
        "scaler_file = f\"{path}/scaler.joblib\"\n",
        "scaler = load(scaler_file)\n",
        "\n",
        "# Load the selector\n",
        "selector_file = f\"{path}/selector.joblib\"\n",
        "selector = load(selector_file)\n",
        "\n",
        "# Load the RNN model\n",
        "rnn_model_file = f\"{path}/rnn_model.keras\"\n",
        "rnn_model = keras.models.load_model(rnn_model_file)\n",
        "\n",
        "print(\"Imputer, LabelEncoder, RNN model, Scaler, and Selector loaded successfully.\")"
      ],
      "metadata": {
        "id": "X_Rqa8AWw5ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baca Dataset dan Preprocessing"
      ],
      "metadata": {
        "id": "1exlJoMIF73G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk membaca dan preprocessing setiap file\n",
        "def read_and_clean_file(file_path):\n",
        "    print(f\"Membaca file: {file_path}\")\n",
        "    df = pd.read_csv(file_path, low_memory=False, sep=\",\")\n",
        "\n",
        "    # Bersihkan nama kolom dari whitespace\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    # Hapus kolom yang tidak diperlukan\n",
        "    redundant_column = ['Flow ID', 'Source IP', 'Source Port', 'Destination IP',\n",
        "                 'Destination Port', 'Protocol', 'Timestamp']\n",
        "    df = df.drop(redundant_column, axis=1, errors='ignore')\n",
        "\n",
        "    # drop baris yang tidak punya label\n",
        "    df.dropna(subset = ['Label'], inplace=True)\n",
        "\n",
        "    # Handling missing values dan infinite values\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Baca semua file CSV dari folder\n",
        "data1 = dataset_path + \"/Monday-WorkingHours.pcap_ISCX.csv\"\n",
        "data2 = dataset_path + \"/Tuesday-WorkingHours.pcap_ISCX.csv\"\n",
        "data3 = dataset_path + \"/Wednesday-workingHours.pcap_ISCX.csv\"\n",
        "data4 = dataset_path + \"/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\"\n",
        "data5 = dataset_path + \"/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\"\n",
        "data6 = dataset_path + \"/Friday-WorkingHours-Morning.pcap_ISCX.csv\"\n",
        "data7 = dataset_path + \"/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\"\n",
        "data8 = dataset_path + \"/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"\n",
        "\n",
        "\n",
        "# Buat list semua dataset yang tersedia\n",
        "all_files = [data2, data3, data4, data5, data6, data7, data8]\n",
        "\n",
        "# Membaca file dan mengkonversi semua data file dari list \"all_files\" menjadi dataframe\n",
        "dataframes = []\n",
        "for file in all_files:\n",
        "    df = read_and_clean_file(file)\n",
        "    dataframes.append(df)\n",
        "    del df\n",
        "\n",
        "# Menggabungkan semua dataframe\n",
        "print(\"Menggabungkan semua file...\")\n",
        "df = pd.concat(dataframes, ignore_index=True)\n",
        "try:\n",
        "    print(\"Semua file dataset berhasil digabungkan!\")\n",
        "except:\n",
        "    print(\"Error! file dataset tidak berhasil digabungkan\")\n",
        "\n",
        "# ganti nama kolom dengan cara hapus whitespaces\n",
        "col_names = {col: col.strip() for col in df.columns}\n",
        "df.rename(columns = col_names, inplace = True)\n",
        "\n",
        "# informasi data duplikat\n",
        "dups = df[df.duplicated()]\n",
        "print(f'Banyak data duplikat : {len(dups)}')\n",
        "print(f'Banyak data sebelum duplikat : {df.shape[0]}')\n",
        "\n",
        "print(\"menghapus data duplikat...\")\n",
        "\n",
        "# Hapus data duplikat\n",
        "df.drop_duplicates(inplace = True)\n",
        "print(\"data duplikat selesai dihapus!\")\n",
        "df.shape\n",
        "print(f\"banyak data setelah data duplikat dihapus : {df.shape[0]}\")\n",
        "\n",
        "# konversi semua label selain BENIGN jadi ATTACK\n",
        "df[\"Label\"] = df[\"Label\"].where(df[\"Label\"] == \"BENIGN\", \"ATTACK\")\n",
        "print(\"Informasi Kelas : \")\n",
        "df[\"Label\"].unique()\n",
        "\n",
        "\n",
        "# Menampilkan informasi dataset\n",
        "print(\"\\nInformasi Dataset:\")\n",
        "print(f\"\\nJumlah total data: {len(df)}\")\n",
        "print(f\"Jumlah fitur : {len(df.columns)}\")\n",
        "print(\"\\nDistribusi Label sebelum preprocessing:\")\n",
        "\n",
        "# tabel distribusi label\n",
        "def create_distribution_table(df):\n",
        "    label_dist = pd.DataFrame(df['Label'].value_counts())\n",
        "    label_dist['percentage'] = df['Label'].value_counts()/len(df)\n",
        "    return label_dist\n",
        "\n",
        "create_distribution_table(df)"
      ],
      "metadata": {
        "id": "tuqsoKC9y3Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Dataset menjadi X dan Y serta Training dan Testing"
      ],
      "metadata": {
        "id": "kwlHhWZYGTGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
        "X = df[numerical_columns]\n",
        "y = df[\"Label\"]\n",
        "print(f\"jumlah fitur : {len(X.columns)}\")\n",
        "print(f\"jumlah label : {len(y.unique())}\")\n",
        "\n",
        "tss = TimeSeriesSplit(n_splits=7)\n",
        "print(tss)\n",
        "\n",
        "#X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
        "train_index, test_index = [], []\n",
        "for i, (train_interval, test_interval) in enumerate(tss.split(X)):\n",
        "    print(f\"fold {i}:\")\n",
        "    print(f\"  Train: index : from {train_interval.min()} up to {train_interval.max()}\")\n",
        "    print(f\"  Test:  index=from {test_interval.min()} up to {test_interval.max()}\")\n",
        "    print(f\"  Jumlah kelas pada training set : {y.iloc[train_interval].nunique()}\")\n",
        "    print(f\"  Jumlah kelas pada testing set : {y.iloc[test_interval].nunique()}\")\n",
        "    train_index, test_index = train_interval, test_interval\n",
        "\n",
        "# Split dataset dengan stratifikasi\n",
        "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "y_train, y_test  = y.iloc[train_index], y.iloc[test_index]"
      ],
      "metadata": {
        "id": "_f2BUMNsFQPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformasi Data Mentah"
      ],
      "metadata": {
        "id": "os3kJU4qGdef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(X, y, scaler, imputer, le):\n",
        "    # Handling missing values untuk dataset training\n",
        "    print(\"\\nMenangani missing values...\")\n",
        "    X = imputer.transform(X)\n",
        "    print(\"selesai!\")\n",
        "\n",
        "    # Normalisasi Data\n",
        "    print(\"\\nMelakukan normalisasi data...\")\n",
        "    X = scaler.transform(X)\n",
        "    print(\"selesai!\")\n",
        "\n",
        "    # Pelabelan Kelas\n",
        "    num_classes = len(le.classes_)\n",
        "    print(\"\\nMelakukan one-hot encoding...\")\n",
        "    y = le.transform(y)\n",
        "    print(\"selesai!\")\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "t1vsOxUCFba5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformasi Data Training\n",
        "X_train, y_train = transform_data(X_train, y_train, scaler, imputer, label_encoder)\n",
        "\n",
        "# Transformmasi data testing\n",
        "X_test, y_test = transform_data(X_test, y_test, scaler, imputer, label_encoder)\n",
        "\n",
        "# Select feature untuk data training juga\n",
        "X_train_selected = selector.transform(X_train)\n",
        "\n",
        "# Select feature untuk data testing\n",
        "X_test_selected = selector.transform(X_test)"
      ],
      "metadata": {
        "id": "kGnFEsCIFwDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluasi Model"
      ],
      "metadata": {
        "id": "T7KgoTWMGmmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = X_train_selected.shape[1]\n",
        "num_classes = len(label_encoder.classes_)\n",
        "num_classes"
      ],
      "metadata": {
        "id": "sUSrr-901dIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, targets, timesteps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - timesteps):\n",
        "        X.append(data[i:i+timesteps])  # Ambil blok sekuensial\n",
        "        y.append(targets[i+timesteps]) # Target berikutnya\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Pilih jumlah timestep (contoh: 10 langkah waktu)\n",
        "timesteps = 10\n",
        "\n",
        "# Buat sequence untuk training dan testing\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_selected, y_train, timesteps)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test_selected, y_test, timesteps)"
      ],
      "metadata": {
        "id": "SICS5HZp2k8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi model\n",
        "y_pred_prob = rnn_model.predict(X_test_seq)\n",
        "y_pred_classes = (y_pred_prob > 0.5).astype(int)\n",
        "y_test_classes = y_test_seq\n",
        "\n",
        "# Tampilkan hasil evaluasi\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "xJcbVCpc2qqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZm2qs5r5WbZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}